{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mo7amed-Soliman/Chatbot-for-Cancer/blob/main/Oncologist_Assistant_Doctor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83NLHqU_XG2F"
      },
      "source": [
        "\n",
        "\n",
        "# **AI-Powered Oncologist ChatBot for Cancer Disease Support**\n",
        "## **Overview**\n",
        "\n",
        "Oncologist Assistant Doctor is an advanced AI chatbot designed to answer medical queries related to Cancer diseases. By leveraging Groq's language model (LLM) and a custom medical knowledge base, this system can provide accurate medical answers, powered by documents extracted from trusted medical websites.\n",
        "\n",
        "This guide walks you through the implementation of the chatbot, integration with various APIs, and setting up a user-friendly interface using **Gradio** for real-time interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code"
      ],
      "metadata": {
        "id": "tgBvz4c5cCSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ngrok\n",
        "!pip install pyngrok nest_asyncio fastapi uvicorn\n",
        "!pip -q install gradio langchain_groq langchain_community chromadb langdetect\n",
        "!pip -q install  pyngrok flask_ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "21ZYqEoLcFXx",
        "outputId": "c515303a-6465-4870-a114-2635858f827d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ngrok\n",
            "Successfully installed ngrok-1.4.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.5 starlette-0.46.2 uvicorn-0.34.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2wFD0C31ZrnW5JPRgPqf9ZRbWHu_2N8zHXNSHzAQWwvrsFDq\n",
        "### Must Change it from here https://dashboard.ngrok.com/get-started/setup/windows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux2H2KH2cFbY",
        "outputId": "e74f0a01-d9a8-493a-d8f6-aa32cce9285b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "###########\n",
        "\n",
        "## 3\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import pickle\n",
        "import json\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "#######\n",
        "\n",
        "GROQ_API_KEY = \"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\" ### change from https://console.groq.com/keys\n",
        "\n",
        "\n",
        "\n",
        "class  OncologistChatbot:\n",
        "    \"\"\"\n",
        "    A chatbot designed to answer questions related to eye diseases using Groq's LLM\n",
        "    and a custom medical knowledge base built from web content.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, groq_api_key):\n",
        "        \"\"\"\n",
        "        Initializes the chatbot with necessary models and configurations.\n",
        "        - Groq's LLM for language processing.\n",
        "        - Hugging Face embedding model for document embeddings.\n",
        "        - Recursive text splitter to divide large documents into smaller chunks.\n",
        "        - ChromaDB for storing and querying processed documents.\n",
        "        \"\"\"\n",
        "        # Initialize Groq model with the provided API key\n",
        "        self.llm = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "        # Initialize Hugging Face embedding model for document embedding\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "        # Initialize a text splitter to break large documents into chunks\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)\n",
        "\n",
        "        # Set the directory where ChromaDB will persist its data\n",
        "        self.persist_directory = \"chroma_db\"\n",
        "\n",
        "        # Process web content to create a vector database for document retrieval\n",
        "        self.vector_db = self.process_web_content()\n",
        "\n",
        "    def process_web_content(self):\n",
        "        \"\"\"\n",
        "        Processes web content from a list of medical websites to build a knowledge base.\n",
        "        - Loads content from predefined websites.\n",
        "        - Splits documents into smaller chunks.\n",
        "        - Creates a vector database (ChromaDB) for document similarity searches.\n",
        "        \"\"\"\n",
        "        # List of trusted medical websites for content scraping\n",
        "        medical_sites = [\n",
        "            \"https://www.webmd.com/\",\n",
        "            \"https://www.mayoclinic.org/diseases-conditions/\",\n",
        "            \"https://medlineplus.gov/\",\n",
        "            \"https://www.healthline.com/health\",\n",
        "            \"https://www.cdc.gov/diseasesconditions/\",\n",
        "        ]\n",
        "\n",
        "        all_documents = []  # List to store all loaded documents\n",
        "\n",
        "        # Iterate through each medical site and load documents\n",
        "        for site in medical_sites:\n",
        "            try:\n",
        "                # Initialize a web loader for the site\n",
        "                loader = WebBaseLoader(site)\n",
        "                # Load documents from the site\n",
        "                documents = loader.load()\n",
        "                # Add loaded documents to the all_documents list\n",
        "                all_documents.extend(documents)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Failed to load data from {site}: {e}\")\n",
        "\n",
        "        # If no documents are loaded, raise an error\n",
        "        if not all_documents:\n",
        "            raise ValueError(\"❌ No data could be loaded from the websites.\")\n",
        "\n",
        "        # Split the documents into smaller chunks for easier processing\n",
        "        chunks = self.text_splitter.split_documents(all_documents)\n",
        "\n",
        "        # Create and return a Chroma vector database for document retrieval\n",
        "        return Chroma.from_documents(chunks, self.embedding_model, persist_directory=self.persist_directory)\n",
        "\n",
        "    def generate_answer(self, question, history):\n",
        "        \"\"\"\n",
        "        Generates an answer to the user's question based on available medical knowledge and chat history.\n",
        "        - Detects the language of the question.\n",
        "        - Searches for relevant documents in the vector database.\n",
        "        - Uses a prompt template to generate an answer based on the context.\n",
        "        \"\"\"\n",
        "        from langdetect import detect  # Import language detection module\n",
        "\n",
        "        # Detect the language of the user's question\n",
        "        question_language = detect(question)\n",
        "\n",
        "        # Retrieve similar documents from the vector database\n",
        "        similar_docs = self.vector_db.similarity_search(question, k=2)\n",
        "\n",
        "        # Combine the content of the similar documents to form context\n",
        "        context = \"\\n\".join([doc.page_content for doc in similar_docs]) if similar_docs else None\n",
        "\n",
        "        # Format the chat history for use in the prompt template\n",
        "        history_text = \"\\n\".join([f\"User: {q}\\nBot: {a}\" for q, a in history]) if history else \"No prior history.\"\n",
        "\n",
        "        # Define a prompt template for question answering\n",
        "        qna_template = \"\"\"You are an Oncologist specializing in Cancer.\n",
        "You answer questions **in the same language as the input question only (Arabic or English) ** ({language}), based on your medical knowledge, the available context, and chat history.\n",
        "- If no answer is available in the context, respond in the same language as the question:\n",
        "**English:** \"No answer is currently available.\"\n",
        "**Arabic:** \"لا يوجد إجابة متاحة حاليًا.\"\n",
        "- Analyze the question medically before answering, relying on reliable scientific information.\n",
        "- Keep your answers precise and to the point, avoiding unnecessary details.\n",
        "- Provide additional advice if requested.\n",
        "### Medical Context:\n",
        "{context}\n",
        "\n",
        "### Chat History:\n",
        "{history}\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer (in the same language  only (Arabic or English) {language}):\"\"\"\n",
        "\n",
        "\n",
        "        # Create the prompt template object\n",
        "        qna_prompt = PromptTemplate(\n",
        "            template=qna_template,\n",
        "            input_variables=['context', 'question', 'language', 'history']\n",
        "        )\n",
        "\n",
        "        # Load the QA chain using the Groq model and the defined prompt\n",
        "        stuff_chain = load_qa_chain(self.llm, chain_type=\"stuff\", prompt=qna_prompt)\n",
        "\n",
        "        # If no context is found, provide a default message based on the question's language\n",
        "        if not context:\n",
        "            output = \"No answer is currently available.\" if question_language == \"en\" else \"لا يوجد إجابة متاحة حاليًا.\"\n",
        "        else:\n",
        "            # Generate the answer using the QA chain and context\n",
        "            answer_generator = stuff_chain.stream({\n",
        "                \"input_documents\": similar_docs,\n",
        "                \"question\": question,\n",
        "                \"language\": question_language,\n",
        "                \"history\": history_text\n",
        "            })\n",
        "\n",
        "            # Accumulate the output from the answer generator\n",
        "            output = \"\"\n",
        "            for chunk in answer_generator:\n",
        "                output += chunk[\"output_text\"]\n",
        "\n",
        "        # Format the output text based on the question's language (Arabic or English)\n",
        "\n",
        "            return {\n",
        "                 \"answer\": output,\n",
        "                  \"language\": question_language\n",
        "                 }\n",
        "\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "## 4 and ngrok\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "chatbot_instance = OncologistChatbot(groq_api_key=GROQ_API_KEY)\n",
        "chat_history = []\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask_question(data: QuestionRequest):\n",
        "    try:\n",
        "        response = chatbot_instance.generate_answer(data.question, chat_history)\n",
        "        chat_history.append((data.question, response[\"answer\"]))\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv05305zcFem",
        "outputId": "84a575d4-a2b6-4166-c86e-a76e0a628596"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-69' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://ce39-34-16-189-245.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [758]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [758]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
